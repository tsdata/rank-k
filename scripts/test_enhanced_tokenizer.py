#!/usr/bin/env python3
"""
Test script for enhanced Korean tokenizer functionality.

Tests the new features:
1. Custom word addition with setup_korean_tokenizer function
2. POS filtering capabilities 
3. Enhanced KiwiTokenizer class methods
"""

from ranx_k.tokenizers.kiwi_tokenizer import setup_korean_tokenizer, korean_tokenizer_function, KiwiTokenizer

def test_basic_functionality():
    """Test basic tokenizer functionality."""
    print("ğŸ§ª Test 1: Basic Functionality | ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # Test text
    test_text = "ë¦¬ë¹„ì•ˆì€ ì–¸ì œ ì„¤ë¦½ë˜ì—ˆë‚˜ìš”?"
    
    # Basic tokenizer
    tokenizer = KiwiTokenizer(method='morphs')
    tokens = tokenizer.tokenize(test_text)
    
    print(f"ğŸ“ Original text | ì›ë¬¸: {test_text}")
    print(f"ğŸ”¤ Basic tokens | ê¸°ë³¸ í† í°: {tokens}")
    print()

def test_setup_korean_tokenizer():
    """Test the setup_korean_tokenizer function."""
    print("ğŸ§ª Test 2: setup_korean_tokenizer Function | setup_korean_tokenizer í•¨ìˆ˜ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # Custom words to add
    custom_words = [
        ('ë¦¬ë¹„ì•ˆ', 'NNP'),  # Proper noun
        ('í…ŒìŠ¬ë¼', 'NNP'),  # Proper noun
        ('ì „ê¸°ì°¨', 'NNG'),  # General noun
        ('ììœ¨ì£¼í–‰', 'NNG'),  # General noun
    ]
    
    # Setup tokenizer with custom words
    tokenizer = setup_korean_tokenizer(
        custom_words=custom_words,
        method='morphs',
        pos_filter=['N', 'V']  # Only nouns and verbs
    )
    
    # Test texts
    test_texts = [
        "ë¦¬ë¹„ì•ˆì€ ì–¸ì œ ì„¤ë¦½ë˜ì—ˆë‚˜ìš”?",
        "í…ŒìŠ¬ë¼ì˜ ì „ê¸°ì°¨ ë¼ì¸ì—…ì—ì„œ Model XëŠ” ì–´ë–¤ ìœ„ì¹˜ë¥¼ ì°¨ì§€í•˜ë‚˜ìš”?",
        "ììœ¨ì£¼í–‰ ê¸°ìˆ ì€ ì–´ë–»ê²Œ ë°œì „í•˜ê³  ìˆë‚˜ìš”?"
    ]
    
    print("\nğŸ“‹ Test Results | í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    for i, text in enumerate(test_texts, 1):
        tokens = tokenizer.tokenize(text)
        print(f"  {i}. Original | ì›ë¬¸: {text}")
        print(f"     Tokens | í† í°: {tokens}")
        print()

def test_pos_filtering():
    """Test POS filtering functionality."""
    print("ğŸ§ª Test 3: POS Filtering | í’ˆì‚¬ í•„í„°ë§ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    test_text = "ë¦¬ë¹„ì•ˆì€ ì „ê¸°ì°¨ë¥¼ ì œì¡°í•˜ëŠ” í˜ì‹ ì ì¸ íšŒì‚¬ì…ë‹ˆë‹¤."
    
    # Test different POS filters
    pos_filters = [
        (['N'], "Nouns only | ëª…ì‚¬ë§Œ"),
        (['V'], "Verbs only | ë™ì‚¬ë§Œ"),
        (['N', 'V'], "Nouns and Verbs | ëª…ì‚¬ì™€ ë™ì‚¬"),
        (['N', 'V', 'M'], "Nouns, Verbs, Modifiers | ëª…ì‚¬, ë™ì‚¬, ìˆ˜ì‹ì–¸")
    ]
    
    print(f"ğŸ“ Test text | í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸: {test_text}")
    print()
    
    for pos_filter, description in pos_filters:
        tokenizer = KiwiTokenizer(method='morphs', pos_filter=pos_filter)
        tokens = tokenizer.tokenize(test_text)
        print(f"ğŸ”§ {description}")
        print(f"   Filter | í•„í„°: {pos_filter}")
        print(f"   Tokens | í† í°: {tokens}")
        print()

def test_korean_tokenizer_function():
    """Test the korean_tokenizer_function for compatibility."""
    print("ğŸ§ª Test 4: korean_tokenizer_function Compatibility | korean_tokenizer_function í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # Setup tokenizer
    kiwi_model = setup_korean_tokenizer()
    
    # Test text
    test_text = "ë¦¬ë¹„ì•ˆì€ ì–¸ì œ ì„¤ë¦½ë˜ì—ˆë‚˜ìš”?"
    
    # Use compatibility function
    tokens = korean_tokenizer_function(test_text, kiwi_model)
    
    print(f"ğŸ“ Original | ì›ë¬¸: {test_text}")
    print(f"ğŸ”¤ Tokens | í† í°: {tokens}")
    print()

def test_custom_word_methods():
    """Test custom word addition methods."""
    print("ğŸ§ª Test 5: Custom Word Methods | ì»¤ìŠ¤í…€ ë‹¨ì–´ ë©”ì„œë“œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # Create tokenizer
    tokenizer = KiwiTokenizer(method='morphs')
    
    # Test before adding custom words
    test_text = "BYDì™€ NIOëŠ” ì¤‘êµ­ì˜ ì „ê¸°ì°¨ íšŒì‚¬ì…ë‹ˆë‹¤."
    tokens_before = tokenizer.tokenize(test_text)
    print(f"ğŸ“ Test text | í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸: {test_text}")
    print(f"ğŸ”¤ Before custom words | ì»¤ìŠ¤í…€ ë‹¨ì–´ ì¶”ê°€ ì „: {tokens_before}")
    
    # Add custom words
    custom_words = [('BYD', 'NNP'), ('NIO', 'NNP')]
    tokenizer.add_custom_words(custom_words)
    
    # Test after adding custom words
    tokens_after = tokenizer.tokenize(test_text)
    print(f"ğŸ”¤ After custom words | ì»¤ìŠ¤í…€ ë‹¨ì–´ ì¶”ê°€ í›„: {tokens_after}")
    print()

def main():
    """Run all tests."""
    print("ğŸš€ Enhanced Korean Tokenizer Test Suite | ê°œì„ ëœ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ëª¨ìŒ")
    print("ğŸš€ Testing new features | ìƒˆë¡œìš´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print()
    
    try:
        test_basic_functionality()
        test_setup_korean_tokenizer()
        test_pos_filtering()
        test_korean_tokenizer_function()
        test_custom_word_methods()
        
        print("âœ… All tests completed successfully | ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"âŒ Test failed | í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()