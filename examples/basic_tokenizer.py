#!/usr/bin/env python3
"""
ranx-k ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì‚¬ìš© ì˜ˆì œ

ì´ ì˜ˆì œëŠ” KiwiTokenizerì˜ ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""

from ranx_k.tokenizers import KiwiTokenizer

def main():
    print("ğŸ”¤ ranx-k KiwiTokenizer ì˜ˆì œ")
    print("=" * 50)
    
    # 1. ê¸°ë³¸ í˜•íƒœì†Œ ë¶„ì„ í† í¬ë‚˜ì´ì €
    print("\n1ï¸âƒ£ í˜•íƒœì†Œ ë¶„ì„ í† í¬ë‚˜ì´ì €")
    morphs_tokenizer = KiwiTokenizer(method='morphs', use_stopwords=True)
    
    sample_texts = [
        "ìì—°ì–´ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤.",
        "Kiwi í† í¬ë‚˜ì´ì €ëŠ” í•œêµ­ì–´ ë¶„ì„ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
        "ranx-këŠ” ì •ë³´ ê²€ìƒ‰ í‰ê°€ë¥¼ ìœ„í•œ ë„êµ¬ì…ë‹ˆë‹¤."
    ]
    
    for text in sample_texts:
        tokens = morphs_tokenizer.tokenize(text)
        print(f"ì…ë ¥: {text}")
        print(f"í† í°: {tokens}")
        print(f"í† í° ê°œìˆ˜: {len(tokens)}")
        print("-" * 40)
    
    # 2. ëª…ì‚¬ ì¶”ì¶œ í† í¬ë‚˜ì´ì €
    print("\n2ï¸âƒ£ ëª…ì‚¬ ì¶”ì¶œ í† í¬ë‚˜ì´ì €")
    nouns_tokenizer = KiwiTokenizer(method='nouns', use_stopwords=True)
    
    text = "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ ê¸°ìˆ ì„ í™œìš©í•œ ìì—°ì–´ì²˜ë¦¬ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ í‰ê°€"
    morphs_tokens = morphs_tokenizer.tokenize(text)
    nouns_tokens = nouns_tokenizer.tokenize(text)
    
    print(f"ì…ë ¥: {text}")
    print(f"í˜•íƒœì†Œ ë¶„ì„: {morphs_tokens}")
    print(f"ëª…ì‚¬ ì¶”ì¶œ: {nouns_tokens}")
    
    # 3. ë¶ˆìš©ì–´ ì²˜ë¦¬
    print("\n3ï¸âƒ£ ë¶ˆìš©ì–´ ì»¤ìŠ¤í„°ë§ˆì´ì§•")
    custom_tokenizer = KiwiTokenizer(method='morphs', use_stopwords=True)
    
    text = "ì´ ì‹œìŠ¤í…œì€ ë§¤ìš° íš¨ê³¼ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤."
    
    # ê¸°ë³¸ ë¶ˆìš©ì–´ë¡œ í† í°í™”
    original_tokens = custom_tokenizer.tokenize(text)
    print(f"ì…ë ¥: {text}")
    print(f"ê¸°ë³¸ ë¶ˆìš©ì–´: {original_tokens}")
    
    # ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ ì¶”ê°€
    custom_tokenizer.add_stopwords(['ì‹œìŠ¤í…œ', 'ë°©ë²•'])
    custom_tokens = custom_tokenizer.tokenize(text)
    print(f"ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ ì¶”ê°€ í›„: {custom_tokens}")
    
    # í˜„ì¬ ë¶ˆìš©ì–´ í™•ì¸
    stopwords = custom_tokenizer.get_stopwords()
    print(f"ì „ì²´ ë¶ˆìš©ì–´ ê°œìˆ˜: {len(stopwords)}")
    print(f"ì¼ë¶€ ë¶ˆìš©ì–´: {list(stopwords)[:10]}")
    
    # 4. ë¹„êµ ë¶„ì„
    print("\n4ï¸âƒ£ í† í¬ë‚˜ì´ì € ë¹„êµ")
    comparison_text = "ìì—°ì–´ì²˜ë¦¬ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ AI ì‹œìŠ¤í…œì´ í–¥ìƒë˜ê³  ìˆìŠµë‹ˆë‹¤."
    
    # ë¶ˆìš©ì–´ ì‚¬ìš© vs ë¯¸ì‚¬ìš©
    with_stopwords = KiwiTokenizer(method='morphs', use_stopwords=True)
    without_stopwords = KiwiTokenizer(method='morphs', use_stopwords=False)
    
    tokens_with = with_stopwords.tokenize(comparison_text)
    tokens_without = without_stopwords.tokenize(comparison_text)
    
    print(f"ì…ë ¥: {comparison_text}")
    print(f"ë¶ˆìš©ì–´ ì œê±°: {tokens_with} ({len(tokens_with)}ê°œ)")
    print(f"ë¶ˆìš©ì–´ ìœ ì§€: {tokens_without} ({len(tokens_without)}ê°œ)")
    
    # 5. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    print("\n5ï¸âƒ£ í† í°í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    import time
    
    test_texts = [
        "í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
        "ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ í‰ê°€ ë°©ë²•ë¡ ì„ ì—°êµ¬í•©ë‹ˆë‹¤.",
        "ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ì´ ë†€ëìŠµë‹ˆë‹¤."
    ] * 100  # 300ê°œ ë¬¸ì¥
    
    start_time = time.time()
    total_tokens = 0
    
    for text in test_texts:
        tokens = morphs_tokenizer.tokenize(text)
        total_tokens += len(tokens)
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"ì²˜ë¦¬ ë¬¸ì¥ ìˆ˜: {len(test_texts)}")
    print(f"ì´ í† í° ìˆ˜: {total_tokens}")
    print(f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.3f}ì´ˆ")
    print(f"ì´ˆë‹¹ ë¬¸ì¥ ì²˜ë¦¬: {len(test_texts) / processing_time:.1f}ê°œ")
    print(f"ì´ˆë‹¹ í† í° ì²˜ë¦¬: {total_tokens / processing_time:.1f}ê°œ")
    
    print("\nâœ… ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì˜ˆì œ ì™„ë£Œ!")

if __name__ == "__main__":
    main()