#!/usr/bin/env python3
"""
ranx-k Basic Tokenizer Usage Example

This example demonstrates the basic usage of KiwiTokenizer.
"""

from ranx_k.tokenizers import KiwiTokenizer

def main():
    print("ğŸ”¤ ranx-k KiwiTokenizer Example | ranx-k KiwiTokenizer ì˜ˆì œ")
    print("=" * 50)
    
    # 1. Basic morphological analysis tokenizer
    print("\n1ï¸âƒ£ Morphological Analysis Tokenizer | í˜•íƒœì†Œ ë¶„ì„ í† í¬ë‚˜ì´ì €")
    morphs_tokenizer = KiwiTokenizer(method='morphs', use_stopwords=True)
    
    sample_texts = [
        "ìì—°ì–´ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤.",
        "Kiwi í† í¬ë‚˜ì´ì €ëŠ” í•œêµ­ì–´ ë¶„ì„ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
        "ranx-këŠ” ì •ë³´ ê²€ìƒ‰ í‰ê°€ë¥¼ ìœ„í•œ ë„êµ¬ì…ë‹ˆë‹¤."
    ]
    
    for text in sample_texts:
        tokens = morphs_tokenizer.tokenize(text)
        print(f"Input | ì…ë ¥: {text}")
        print(f"Tokens | í† í°: {tokens}")
        print(f"Token Count | í† í° ê°œìˆ˜: {len(tokens)}")
        print("-" * 40)
    
    # 2. Noun extraction tokenizer
    print("\n2ï¸âƒ£ Noun Extraction Tokenizer | ëª…ì‚¬ ì¶”ì¶œ í† í¬ë‚˜ì´ì €")
    nouns_tokenizer = KiwiTokenizer(method='nouns', use_stopwords=True)
    
    text = "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ ê¸°ìˆ ì„ í™œìš©í•œ ìì—°ì–´ì²˜ë¦¬ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ í‰ê°€"
    morphs_tokens = morphs_tokenizer.tokenize(text)
    nouns_tokens = nouns_tokenizer.tokenize(text)
    
    print(f"Input | ì…ë ¥: {text}")
    print(f"Morpheme Analysis | í˜•íƒœì†Œ ë¶„ì„: {morphs_tokens}")
    print(f"Noun Extraction | ëª…ì‚¬ ì¶”ì¶œ: {nouns_tokens}")
    
    # 3. Stopword processing
    print("\n3ï¸âƒ£ Stopword Customization | ë¶ˆìš©ì–´ ì»¤ìŠ¤í„°ë§ˆì´ì§•")
    custom_tokenizer = KiwiTokenizer(method='morphs', use_stopwords=True)
    
    text = "ì´ ì‹œìŠ¤í…œì€ ë§¤ìš° íš¨ê³¼ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤."
    
    # Tokenization with default stopwords
    original_tokens = custom_tokenizer.tokenize(text)
    print(f"Input | ì…ë ¥: {text}")
    print(f"Default Stopwords | ê¸°ë³¸ ë¶ˆìš©ì–´: {original_tokens}")
    
    # Add custom stopwords
    custom_tokenizer.add_stopwords(['ì‹œìŠ¤í…œ', 'ë°©ë²•'])
    custom_tokens = custom_tokenizer.tokenize(text)
    print(f"After Adding Custom Stopwords | ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ ì¶”ê°€ í›„: {custom_tokens}")
    
    # Check current stopwords
    stopwords = custom_tokenizer.get_stopwords()
    print(f"Total Stopwords Count | ì „ì²´ ë¶ˆìš©ì–´ ê°œìˆ˜: {len(stopwords)}")
    print(f"Sample Stopwords | ì¼ë¶€ ë¶ˆìš©ì–´: {list(stopwords)[:10]}")
    
    # 4. Comparison analysis
    print("\n4ï¸âƒ£ Tokenizer Comparison | í† í¬ë‚˜ì´ì € ë¹„êµ")
    comparison_text = "ìì—°ì–´ì²˜ë¦¬ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ AI ì‹œìŠ¤í…œì´ í–¥ìƒë˜ê³  ìˆìŠµë‹ˆë‹¤."
    
    # Stopwords usage vs non-usage comparison
    with_stopwords = KiwiTokenizer(method='morphs', use_stopwords=True)
    without_stopwords = KiwiTokenizer(method='morphs', use_stopwords=False)
    
    tokens_with = with_stopwords.tokenize(comparison_text)
    tokens_without = without_stopwords.tokenize(comparison_text)
    
    print(f"Input | ì…ë ¥: {comparison_text}")
    print(f"Stopwords Removed | ë¶ˆìš©ì–´ ì œê±°: {tokens_with} ({len(tokens_with)}ê°œ)")
    print(f"Stopwords Kept | ë¶ˆìš©ì–´ ìœ ì§€: {tokens_without} ({len(tokens_without)}ê°œ)")
    
    # 5. Performance test
    print("\n5ï¸âƒ£ Tokenization Performance Test | í† í°í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    import time
    
    test_texts = [
        "í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ëŠ” í˜•íƒœì†Œ ë¶„ì„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
        "ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ í‰ê°€ ë°©ë²•ë¡ ì„ ì—°êµ¬í•©ë‹ˆë‹¤.",
        "ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ì´ ë†€ëìŠµë‹ˆë‹¤."
    ] * 100  # 300 sentences
    
    start_time = time.time()
    total_tokens = 0
    
    for text in test_texts:
        tokens = morphs_tokenizer.tokenize(text)
        total_tokens += len(tokens)
    
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"Processed Sentences | ì²˜ë¦¬ ë¬¸ì¥ ìˆ˜: {len(test_texts)}")
    print(f"Total Tokens | ì´ í† í° ìˆ˜: {total_tokens}")
    print(f"Processing Time | ì²˜ë¦¬ ì‹œê°„: {processing_time:.3f}ì´ˆ")
    print(f"Sentences Per Second | ì´ˆë‹¹ ë¬¸ì¥ ì²˜ë¦¬: {len(test_texts) / processing_time:.1f}ê°œ")
    print(f"Tokens Per Second | ì´ˆë‹¹ í† í° ì²˜ë¦¬: {total_tokens / processing_time:.1f}ê°œ")
    
    print("\nâœ… Basic Tokenizer Example Completed | ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì˜ˆì œ ì™„ë£Œ!")

if __name__ == "__main__":
    main()