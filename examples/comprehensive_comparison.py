#!/usr/bin/env python3
"""
ranx-k Comprehensive Evaluation Comparison Example

ì´ ì˜ˆì œëŠ” Comprehensively compares and analyzes all evaluation methods.
"""

import time
import numpy as np
from typing import List, Dict, Any
from ranx_k.evaluation import (
    simple_kiwi_rouge_evaluation,
    rouge_kiwi_enhanced_evaluation,
    evaluate_with_ranx_similarity,
    comprehensive_evaluation_comparison
)
from ranx_k.tokenizers import KiwiTokenizer

class Document:
    def __init__(self, content: str, doc_id: str = None):
        self.page_content = content
        self.doc_id = doc_id or str(hash(content))

class ComprehensiveRetriever:
    """Advanced retriever for comprehensive evaluation"""
    
    def __init__(self, documents: List[str]):
        self.documents = [Document(doc, f"doc_{i}") for i, doc in enumerate(documents)]
        self.tokenizer = KiwiTokenizer(method='morphs', use_stopwords=True)
        
    def invoke(self, query: str, top_k: int = 10) -> List[Document]:
        """TF-IDF based search"""
        query_tokens = self.tokenizer.tokenize(query)
        
        # Calculate relevance score for each document
        doc_scores = []
        
        for doc in self.documents:
            doc_tokens = self.tokenizer.tokenize(doc.page_content)
            
            # Simple TF-IDF score calculation
            score = 0
            for token in query_tokens:
                if token in doc_tokens:
                    tf = doc_tokens.count(token) / len(doc_tokens)
                    score += tf
            
            if score > 0:
                doc_scores.append((score, doc))
        
        # Sort by score
        doc_scores.sort(key=lambda x: x[0], reverse=True)
        return [doc for score, doc in doc_scores[:top_k]]

def create_evaluation_dataset():
    """Create evaluation dataset"""
    
    # Domain-specific document collection
    documents = [
        # Natural language processing related
        "ìì—°ì–´ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  ë¶„ì•¼ì…ë‹ˆë‹¤.",
        "í˜•íƒœì†Œ ë¶„ì„ì€ í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ì˜ í•µì‹¬ ê¸°ìˆ ë¡œ ë‹¨ì–´ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•´í•©ë‹ˆë‹¤.",
        "í† í°í™” ê³¼ì •ì—ì„œ í•œêµ­ì–´ì˜ êµì°©ì–´ì  íŠ¹ì„±ì„ ê³ ë ¤í•´ì•¼ ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        
        # RAG system related  
        "RAGëŠ” Retrieval-Augmented Generationì˜ ì¤„ì„ë§ë¡œ ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "ê²€ìƒ‰ ì¦ê°• ìƒì„± ì‹œìŠ¤í…œì€ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•„ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.",
        "RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì€ ê²€ìƒ‰ í’ˆì§ˆê³¼ ìƒì„± í’ˆì§ˆì— ëª¨ë‘ ì˜ì¡´í•©ë‹ˆë‹¤.",
        
        # Information retrieval related
        "ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì€ ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜ë¡œ í‰ê°€ë©ë‹ˆë‹¤.",
        "ê²€ìƒ‰ ì—”ì§„ì€ ì‚¬ìš©ì ì¿¼ë¦¬ì— ë§ëŠ” ê´€ë ¨ ë¬¸ì„œë¥¼ ë¹ ë¥´ê²Œ ì°¾ì•„ ì œê³µí•©ë‹ˆë‹¤.",
        "ë²¡í„° ê²€ìƒ‰ì€ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.",
        
        # Evaluation metrics related
        "ROUGE ë©”íŠ¸ë¦­ì€ ìš”ì•½ë¬¸ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ëŒ€í‘œì ì¸ ìë™ í‰ê°€ ë°©ë²•ì…ë‹ˆë‹¤.",
        "NDCGëŠ” ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœìœ„ í’ˆì§ˆì„ ì¸¡ì •í•˜ëŠ” ì •ë³´ ê²€ìƒ‰ í‰ê°€ ì§€í‘œì…ë‹ˆë‹¤.",
        "Hit@KëŠ” ìƒìœ„ Kê°œ ê²°ê³¼ ì¤‘ ì •ë‹µì´ í¬í•¨ëœ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚´ëŠ” ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤.",
        
        # Machine learning related
        "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€ì—ëŠ” ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ê³¼ ê²€ì¦ ë°©ë²•ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.",
        "êµì°¨ ê²€ì¦ì€ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤.",
        "ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì •ê·œí™” ê¸°ë²•ê³¼ ì¡°ê¸° ì¢…ë£Œë¥¼ í™œìš©í•©ë‹ˆë‹¤.",
        
        # Deep learning related
        "íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ëŠ” ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ì— í˜ì‹ ì„ ê°€ì ¸ì˜¨ ì‹ ê²½ë§ êµ¬ì¡°ì…ë‹ˆë‹¤.",
        "ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.",
        "ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ í•˜ìœ„ ì‘ì—…ì— íŒŒì¸íŠœë‹í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    ]
    
    # Question-answer pairs for evaluation
    questions = [
        "ìì—°ì–´ì²˜ë¦¬ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í•œêµ­ì–´ í† í°í™”ì˜ íŠ¹ì§•ì€?", 
        "RAG ì‹œìŠ¤í…œì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?",
        "ì •ë³´ ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ë°©ë²•ì€?",
        "ROUGE ë©”íŠ¸ë¦­ì˜ íŠ¹ì§•ì€?",
        "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í‰ê°€ ë°©ë²•ì€?",
        "íŠ¸ëœìŠ¤í¬ë¨¸ì˜ íŠ¹ì§•ì€?",
        "ê²€ìƒ‰ ì—”ì§„ì˜ ì—­í• ì€?"
    ]
    
    # Correct documents for each question
    reference_contexts = [
        ["ìì—°ì–´ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  ë¶„ì•¼ì…ë‹ˆë‹¤."],
        ["í† í°í™” ê³¼ì •ì—ì„œ í•œêµ­ì–´ì˜ êµì°©ì–´ì  íŠ¹ì„±ì„ ê³ ë ¤í•´ì•¼ ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."],
        ["RAGëŠ” Retrieval-Augmented Generationì˜ ì¤„ì„ë§ë¡œ ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ ê¸°ìˆ ì…ë‹ˆë‹¤."],
        ["ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì€ ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜ë¡œ í‰ê°€ë©ë‹ˆë‹¤."],
        ["ROUGE ë©”íŠ¸ë¦­ì€ ìš”ì•½ë¬¸ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ëŒ€í‘œì ì¸ ìë™ í‰ê°€ ë°©ë²•ì…ë‹ˆë‹¤."],
        ["ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€ì—ëŠ” ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ê³¼ ê²€ì¦ ë°©ë²•ì´ ì‚¬ìš©ë©ë‹ˆë‹¤."],
        ["íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ëŠ” ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ì— í˜ì‹ ì„ ê°€ì ¸ì˜¨ ì‹ ê²½ë§ êµ¬ì¡°ì…ë‹ˆë‹¤."],
        ["ê²€ìƒ‰ ì—”ì§„ì€ ì‚¬ìš©ì ì¿¼ë¦¬ì— ë§ëŠ” ê´€ë ¨ ë¬¸ì„œë¥¼ ë¹ ë¥´ê²Œ ì°¾ì•„ ì œê³µí•©ë‹ˆë‹¤."]
    ]
    
    return documents, questions, reference_contexts

def detailed_performance_analysis(results: Dict[str, Any]):
    """Detailed performance analysis"""
    
    analysis = {
        'rouge_scores': {},
        'ranx_metrics': {},
        'overall_performance': {}
    }
    
    # Extract ROUGE scores
    for method, metrics in results.items():
        rouge_scores = {}
        ranx_scores = {}
        
        for metric_name, score in metrics.items():
            if 'rouge' in metric_name.lower():
                rouge_type = 'rouge1' if 'rouge1' in metric_name.lower() else \
                           'rouge2' if 'rouge2' in metric_name.lower() else \
                           'rougeL' if 'rougel' in metric_name.lower() else 'other'
                rouge_scores[rouge_type] = score
            
            elif any(x in metric_name.lower() for x in ['hit_rate', 'ndcg', 'map', 'mrr']):
                ranx_scores[metric_name] = score
        
        if rouge_scores:
            analysis['rouge_scores'][method] = rouge_scores
        if ranx_scores:
            analysis['ranx_metrics'][method] = ranx_scores
    
    return analysis

def generate_recommendations(analysis: Dict[str, Any]) -> List[str]:
    """Generate recommendations based on result analysis"""
    
    recommendations = []
    
    # ROUGE score-based recommendations
    if 'rouge_scores' in analysis:
        best_rouge_method = None
        best_rouge_score = 0
        
        for method, scores in analysis['rouge_scores'].items():
            avg_score = np.mean(list(scores.values()))
            if avg_score > best_rouge_score:
                best_rouge_score = avg_score
                best_rouge_method = method
        
        if best_rouge_method:
            if best_rouge_score > 0.6:
                recommendations.append(f"âœ… {best_rouge_method} shows the highest ROUGE performance ({best_rouge_score:.3f}).")
            elif best_rouge_score > 0.4:
                recommendations.append(f"âš ï¸ {best_rouge_method} is relatively good ({best_rouge_score:.3f}), but has room for improvement.")
            else:
                recommendations.append(f"ğŸ”´ All methods have low ROUGE scores. System review is required.")
    
    # ranx metric-based recommendations
    if 'ranx_metrics' in analysis:
        recommendations.append("ğŸ“Š Traditional IR evaluation through ranx metrics is available.")
    
    # General recommendations
    recommendations.extend([
        "ğŸ”§ Experiment with tokenization methods (morphs vs nouns).",
        "ğŸ¯ Optimize performance by adjusting similarity_threshold values.",
        "âš¡ Consider batch processing for large datasets.",
        "ğŸ“ˆ Make comprehensive judgments by combining multiple evaluation methods."
    ])
    
    return recommendations

def main():
    print("ğŸ† ranx-k Comprehensive Evaluation Comparison Example | ranx-k ì¢…í•© í‰ê°€ ë¹„êµ ì˜ˆì œ")
    print("=" * 60)
    
    # Prepare dataset
    print("ğŸ“Š Preparing Evaluation Dataset | í‰ê°€ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
    documents, questions, reference_contexts = create_evaluation_dataset()
    retriever = ComprehensiveRetriever(documents)
    
    print(f"ğŸ“š Number of Documents | ë¬¸ì„œ ìˆ˜: {len(documents)}")
    print(f"â“ Number of Questions | ì§ˆë¬¸ ìˆ˜: {len(questions)}")
    print(f"ğŸ“ Answer Pairs | ì •ë‹µ ìŒ: {len(reference_contexts)}")
    
    # 1. Execute individual evaluation methods
    print("\n1ï¸âƒ£ Individual Evaluation Methods Execution | ê°œë³„ í‰ê°€ ë°©ë²• ì‹¤í–‰")
    print("-" * 40)
    
    evaluation_results = {}
    execution_times = {}
    
    # Simple Kiwi ROUGE
    print("ğŸ”¤ Simple Kiwi ROUGE Evaluation | Simple Kiwi ROUGE í‰ê°€...")
    start_time = time.time()
    try:
        simple_results = simple_kiwi_rouge_evaluation(
            retriever=retriever,
            questions=questions,
            reference_contexts=reference_contexts,
            k=5
        )
        evaluation_results['Simple Kiwi ROUGE'] = simple_results
        execution_times['Simple Kiwi ROUGE'] = time.time() - start_time
        print(f"   âœ… Completed | ì™„ë£Œ ({execution_times['Simple Kiwi ROUGE']:.2f}ì´ˆ)")
    except Exception as e:
        print(f"   âŒ Error | ì˜¤ë¥˜: {str(e)}")
    
    # Enhanced ROUGE (morphs)
    print("ğŸ”¬ Enhanced ROUGE (morphs) Evaluation | Enhanced ROUGE (morphs) í‰ê°€...")
    start_time = time.time()
    try:
        enhanced_results = rouge_kiwi_enhanced_evaluation(
            retriever=retriever,
            questions=questions,
            reference_contexts=reference_contexts,
            k=5,
            tokenize_method='morphs'
        )
        evaluation_results['Enhanced ROUGE (morphs)'] = enhanced_results
        execution_times['Enhanced ROUGE (morphs)'] = time.time() - start_time
        print(f"   âœ… Completed | ì™„ë£Œ ({execution_times['Enhanced ROUGE (morphs)']:.2f}ì´ˆ)")
    except Exception as e:
        print(f"   âŒ Error | ì˜¤ë¥˜: {str(e)}")
    
    # Enhanced ROUGE (nouns)
    print("ğŸ”¬ Enhanced ROUGE (nouns) Evaluation | Enhanced ROUGE (nouns) í‰ê°€...")
    start_time = time.time()
    try:
        enhanced_nouns_results = rouge_kiwi_enhanced_evaluation(
            retriever=retriever,
            questions=questions,
            reference_contexts=reference_contexts,
            k=5,
            tokenize_method='nouns'
        )
        evaluation_results['Enhanced ROUGE (nouns)'] = enhanced_nouns_results
        execution_times['Enhanced ROUGE (nouns)'] = time.time() - start_time
        print(f"   âœ… Completed | ì™„ë£Œ ({execution_times['Enhanced ROUGE (nouns)']:.2f}ì´ˆ)")
    except Exception as e:
        print(f"   âŒ Error | ì˜¤ë¥˜: {str(e)}")
    
    # ranx Similarity (Kiwi ROUGE)
    print("ğŸ“Š ranx Similarity (Kiwi ROUGE) Evaluation | ranx Similarity (Kiwi ROUGE) í‰ê°€...")
    start_time = time.time()
    try:
        ranx_results = evaluate_with_ranx_similarity(
            retriever=retriever,
            questions=questions,
            reference_contexts=reference_contexts,
            k=5,
            method='kiwi_rouge',
            similarity_threshold=0.5
        )
        evaluation_results['ranx Similarity (Kiwi ROUGE)'] = ranx_results
        execution_times['ranx Similarity (Kiwi ROUGE)'] = time.time() - start_time
        print(f"   âœ… Completed | ì™„ë£Œ ({execution_times['ranx Similarity (Kiwi ROUGE)']:.2f}ì´ˆ)")
    except Exception as e:
        print(f"   âŒ Error | ì˜¤ë¥˜: {str(e)}")
    
    # 2. Execute comprehensive comparison
    print("\n2ï¸âƒ£ Comprehensive Comparison Evaluation Execution | ì¢…í•© ë¹„êµ í‰ê°€ ì‹¤í–‰")
    print("-" * 30)
    
    start_time = time.time()
    try:
        comprehensive_results = comprehensive_evaluation_comparison(
            retriever=retriever,
            questions=questions,
            reference_contexts=reference_contexts,
            k=5
        )
        comprehensive_time = time.time() - start_time
        print(f"âœ… Comprehensive Comparison Completed | ì¢…í•© ë¹„êµ ì™„ë£Œ ({comprehensive_time:.2f}ì´ˆ)")
    except Exception as e:
        print(f"âŒ Comprehensive Comparison Error | ì¢…í•© ë¹„êµ ì˜¤ë¥˜: {str(e)}")
        comprehensive_results = evaluation_results
    
    # 3. Comprehensive result analysis
    print("\n3ï¸âƒ£ Comprehensive Result Analysis | ê²°ê³¼ ì¢…í•© ë¶„ì„")
    print("=" * 40)
    
    # Performance comparison table
    print("\nğŸ“Š Performance Comparison Table | ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”")
    print("-" * 80)
    print(f"{'Method | ë°©ë²•':<25} {'ROUGE-1':<10} {'ROUGE-2':<10} {'ROUGE-L':<10} {'Time(s) | ì‹¤í–‰ì‹œê°„(s)':<12}")
    print("-" * 80)
    
    for method_name in evaluation_results.keys():
        results = evaluation_results[method_name]
        exec_time = execution_times.get(method_name, 0)
        
        # Extract ROUGE scores
        rouge1 = next((v for k, v in results.items() if 'rouge1' in k.lower()), 0.0)
        rouge2 = next((v for k, v in results.items() if 'rouge2' in k.lower()), 0.0)
        rougeL = next((v for k, v in results.items() if 'rougel' in k.lower()), 0.0)
        
        print(f"{method_name:<25} {rouge1:<10.3f} {rouge2:<10.3f} {rougeL:<10.3f} {exec_time:<12.2f}")
    
    # ranx metrics table
    ranx_methods = [name for name in evaluation_results.keys() if 'ranx' in name.lower()]
    if ranx_methods:
        print(f"\nğŸ“ˆ ranx Metrics Comparison | ranx ë©”íŠ¸ë¦­ ë¹„êµ")
        print("-" * 60)
        print(f"{'Method | ë°©ë²•':<25} {'Hit@5':<10} {'NDCG@5':<10} {'MRR':<10}")
        print("-" * 60)
        
        for method_name in ranx_methods:
            results = evaluation_results[method_name]
            hit_rate = results.get('hit_rate@5', 0.0)
            ndcg = results.get('ndcg@5', 0.0) 
            mrr = results.get('mrr', 0.0)
            
            print(f"{method_name:<25} {hit_rate:<10.3f} {ndcg:<10.3f} {mrr:<10.3f}")
    
    # 4. Detailed analysis
    print("\n4ï¸âƒ£ Detailed Performance Analysis | ìƒì„¸ ì„±ëŠ¥ ë¶„ì„")
    print("-" * 25)
    
    analysis = detailed_performance_analysis(evaluation_results)
    
    # Identify best performance method
    if 'rouge_scores' in analysis and analysis['rouge_scores']:
        print("\nğŸ† ROUGE Performance Rankings | ROUGE ì„±ëŠ¥ ìˆœìœ„:")
        rouge_rankings = []
        
        for method, scores in analysis['rouge_scores'].items():
            avg_score = np.mean(list(scores.values()))
            rouge_rankings.append((method, avg_score))
        
        rouge_rankings.sort(key=lambda x: x[1], reverse=True)
        
        for i, (method, score) in enumerate(rouge_rankings, 1):
            emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "ğŸ“Š"
            print(f"  {emoji} {i}ìœ„ | Rank {i}: {method:<25} ({score:.3f})")
    
    # 5. Generate recommendations
    print("\n5ï¸âƒ£ Recommendations | ê¶Œì¥ì‚¬í•­")
    print("-" * 15)
    
    recommendations = generate_recommendations(analysis)
    for i, rec in enumerate(recommendations, 1):
        print(f"{i:2d}. {rec}")
    
    # 6. Use case-specific recommended methods
    print("\n6ï¸âƒ£ Recommended Methods by Use Case | ì‚¬ìš© ì‚¬ë¡€ë³„ ê¶Œì¥ ë°©ë²•")
    print("-" * 30)
    
    use_cases = [
        ("ğŸš€ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘", "Simple Kiwi ROUGE - ê°€ì¥ ë¹ ë¥´ê³  ê°„ë‹¨"),
        ("ğŸ¢ í”„ë¡œë•ì…˜ í™˜ê²½", "Enhanced ROUGE (morphs) - ì•ˆì •ì ì´ê³  ê²€ì¦ëœ ë°©ë²•"),
        ("ğŸ”¬ ì—°êµ¬/ë²¤ì¹˜ë§ˆí‚¹", "ranx Similarity - ì „í†µì ì¸ IR ë©”íŠ¸ë¦­ ì œê³µ"),
        ("ğŸ¯ ë†’ì€ ì •í™•ë„", "Enhanced ROUGE + ranx ì¡°í•© - ë‹¤ê°ë„ í‰ê°€"),
        ("ğŸ’¾ ë©”ëª¨ë¦¬ ì œì•½", "Kiwi ROUGE (ì„ë² ë”© ë¯¸ì‚¬ìš©) - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ")
    ]
    
    for use_case, recommendation in use_cases:
        print(f"{use_case:<20} â†’ {recommendation}")
    
    # 7. Performance optimization tips
    print("\n7ï¸âƒ£ Performance Optimization Tips | ì„±ëŠ¥ ìµœì í™” íŒ")
    print("-" * 20)
    
    optimization_tips = [
        "ğŸ”§ í† í°í™” ë°©ë²• ì„ íƒ: ì •í™•ë„ëŠ” morphs, ì†ë„ëŠ” nouns",
        "ğŸ¯ ì„ê³„ê°’ ì¡°ì •: similarity_thresholdë¥¼ ë°ì´í„°ì— ë§ê²Œ íŠœë‹",
        "âš¡ ë°°ì¹˜ ì²˜ë¦¬: ëŒ€ëŸ‰ ë°ì´í„°ëŠ” ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬",
        "ğŸ’¾ ìºì‹± í™œìš©: ë°˜ë³µ ì‹¤í–‰ ì‹œ í† í°í™” ê²°ê³¼ ìºì‹±",
        "ğŸ“Š k ê°’ ìµœì í™”: ê²€ìƒ‰ ê¹Šì´ì™€ ì„±ëŠ¥ì˜ ê· í˜•ì  ì°¾ê¸°"
    ]
    
    for tip in optimization_tips:
        print(f"  {tip}")
    
    print(f"\nâœ… Comprehensive Evaluation Comparison Completed | ì¢…í•© í‰ê°€ ë¹„êµ ì™„ë£Œ!")
    print(f"ğŸ“Š Total {len(evaluation_results)} methods evaluated {len(questions)} questions | ì´ {len(evaluation_results)}ê°œ ë°©ë²•ìœ¼ë¡œ {len(questions)}ê°œ ì§ˆë¬¸ í‰ê°€")
    print(f"â±ï¸ Total Execution Time | ì´ ì‹¤í–‰ ì‹œê°„: {sum(execution_times.values()):.2f}ì´ˆ")

if __name__ == "__main__":
    main()