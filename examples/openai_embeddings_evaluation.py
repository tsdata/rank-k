#!/usr/bin/env python3
"""
Korean RAG Evaluation Example Using OpenAI Embeddings

This example demonstrates how to evaluate Korean RAG systems using 
OpenAI's text-embedding-3-small/large models.

Note: Requires OpenAI API key and costs are incurred based on usage.
"""

import os
from ranx_k.evaluation.openai_similarity import (
    evaluate_with_openai_similarity, 
    estimate_openai_cost
)


class SimpleRetriever:
    """Simple test retriever"""
    
    def __init__(self, documents):
        self.documents = documents
    
    def invoke(self, query):
        # In practice, more complex search logic would be implemented here
        return self.documents


def main():
    print("ğŸ¤– Korean RAG Evaluation Example Using OpenAI Embeddings | OpenAI Embeddingsë¥¼ ì‚¬ìš©í•œ í•œêµ­ì–´ RAG í‰ê°€ ì˜ˆì œ")
    print("=" * 60)
    
    # Check API key
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ OPENAI_API_KEY environment variable not set | OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ Setup Instructions | ì„¤ì • ë°©ë²•:")
        print("   export OPENAI_API_KEY='your-api-key-here'")
        print("   or add to .env file | ë˜ëŠ” .env íŒŒì¼ì— ì¶”ê°€í•˜ì„¸ìš”.")
        return
    
    # 1. Prepare test data
    questions = [
        "ìì—°ì–´ì²˜ë¦¬ ê¸°ìˆ ì˜ ìµœì‹  ë™í–¥ì€?",
        "OpenAIì˜ ì„ë² ë”© ëª¨ë¸ íŠ¹ì§•ì€?",
        "í•œêµ­ì–´ AI ëª¨ë¸ ê°œë°œì˜ ì–´ë ¤ì›€ì€?"
    ]
    
    documents = [
        "ìì—°ì–´ì²˜ë¦¬ëŠ” ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìœ¼ë©°, ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ì´ ì£¼ìš” íŠ¸ë Œë“œì…ë‹ˆë‹¤.",
        "OpenAIì˜ ì„ë² ë”© ëª¨ë¸ì€ ë‹¤êµ­ì–´ë¥¼ ì§€ì›í•˜ê³  ë†’ì€ ì •í™•ë„ë¥¼ ìë‘í•©ë‹ˆë‹¤.",
        "í•œêµ­ì–´ AI ê°œë°œì€ ì–¸ì–´ì˜ ë³µì¡ì„±ê³¼ ë°ì´í„° ë¶€ì¡±ì´ ì£¼ìš” ë„ì „ê³¼ì œì…ë‹ˆë‹¤.",
        "RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•˜ì—¬ ë” ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.",
        "ì„ë² ë”© ê¸°ìˆ ì€ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ë²¡í„° ê³µê°„ì—ì„œ ì¸¡ì •í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤."
    ]
    
    reference_contexts = [
        [documents[0]],
        [documents[1]],
        [documents[2]]
    ]
    
    # 2. Cost estimation
    print("\nğŸ’° OpenAI API Cost Estimation | OpenAI API ë¹„ìš© ì¶”ì •:")
    print("-" * 30)
    
    total_texts = len(questions) + len(documents) + sum(len(refs) for refs in reference_contexts)
    
    for model in ["text-embedding-3-small", "text-embedding-3-large"]:
        cost_info = estimate_openai_cost(total_texts, model_name=model)
        print(f"{model}:")
        print(f"  - Estimated Cost | ì˜ˆìƒ ë¹„ìš©: ${cost_info['estimated_cost_usd']} ({cost_info['estimated_cost_krw']}ì›)")
        print(f"  - Total Tokens | ì´ í† í°: {cost_info['total_tokens']:,}")
    
    # 3. User confirmation
    print(f"\nProcessing {total_texts} texts total | ì´ {total_texts}ê°œ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    user_input = input("Continue? | ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    if user_input != 'y':
        print("Evaluation cancelled | í‰ê°€ë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
        return
    
    # 4. Initialize retriever
    retriever = SimpleRetriever(documents)
    
    # 5. OpenAI text-embedding-3-small evaluation
    print("\nğŸ” Evaluating with OpenAI text-embedding-3-small | OpenAI text-embedding-3-smallë¡œ í‰ê°€ ì¤‘...")
    
    try:
        results_small = evaluate_with_openai_similarity(
            retriever=retriever,
            questions=questions,
            reference_contexts=reference_contexts,
            k=3,
            model_name="text-embedding-3-small",
            similarity_threshold=0.7
        )
        
        print("\nğŸ“Š text-embedding-3-small Results | text-embedding-3-small ê²°ê³¼:")
        print("-" * 35)
        for metric, score in results_small.items():
            print(f"{metric:12s}: {score:.4f}")
            
    except Exception as e:
        print(f"âŒ text-embedding-3-small evaluation failed | text-embedding-3-small í‰ê°€ ì‹¤íŒ¨: {e}")
        return
    
    # 6. OpenAI text-embedding-3-large evaluation (optional)
    print("\nğŸ” Evaluate with OpenAI text-embedding-3-large? | OpenAI text-embedding-3-largeë¡œ í‰ê°€í• ê¹Œìš”?")
    user_input = input("Higher performance but more expensive | ë” ë†’ì€ ì„±ëŠ¥ì´ì§€ë§Œ ë¹„ìš©ì´ ë” ë“­ë‹ˆë‹¤. (y/N): ").strip().lower()
    
    if user_input == 'y':
        try:
            results_large = evaluate_with_openai_similarity(
                retriever=retriever,
                questions=questions,
                reference_contexts=reference_contexts,
                k=3,
                model_name="text-embedding-3-large",
                similarity_threshold=0.7
            )
            
            print("\nğŸ“Š text-embedding-3-large Results | text-embedding-3-large ê²°ê³¼:")
            print("-" * 35)
            for metric, score in results_large.items():
                print(f"{metric:12s}: {score:.4f}")
            
            # 7. Performance comparison
            print("\nğŸ“ˆ Model Performance Comparison | ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
            print("-" * 35)
            print(f"{'Metric':<12} {'Small':<8} {'Large':<8} {'Improvement | ê°œì„ ':<8}")
            print("-" * 35)
            
            for metric in results_small.keys():
                small_score = results_small[metric]
                large_score = results_large[metric]
                improvement = ((large_score - small_score) / small_score * 100) if small_score > 0 else 0
                print(f"{metric:<12} {small_score:<8.4f} {large_score:<8.4f} {improvement:+6.1f}%")
                
        except Exception as e:
            print(f"âŒ text-embedding-3-large evaluation failed | text-embedding-3-large í‰ê°€ ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 60)
    print("âœ… OpenAI Embeddings Evaluation Completed | OpenAI Embeddings í‰ê°€ ì™„ë£Œ!")
    print("\nğŸ’¡ Tips | íŒ:")
    print("- Store API key securely in .env file | API í‚¤ë¥¼ .env íŒŒì¼ì— ì €ì¥í•˜ì—¬ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ì„¸ìš”")
    print("- Optimize API calls by adjusting batch size | ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ì—¬ API í˜¸ì¶œ íšŸìˆ˜ë¥¼ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    print("- text-embedding-3-small generally offers best cost-performance ratio | text-embedding-3-smallì´ ì¼ë°˜ì ìœ¼ë¡œ ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤")
    print("- Consider caching for production environments | í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ìºì‹±ì„ ê³ ë ¤í•˜ì„¸ìš”")


def show_openai_embedding_models():
    """Display OpenAI Embedding model information"""
    print("\nğŸ¤– OpenAI Embedding Model Comparison | OpenAI Embedding ëª¨ë¸ ë¹„êµ:")
    print("=" * 50)
    
    models = [
        {
            "name": "text-embedding-3-small",
            "dimensions": 1536,
            "cost_per_1M": "$0.02",
            "performance": "High | ë†’ìŒ",
            "recommended": "âœ… Recommended | ê¶Œì¥"
        },
        {
            "name": "text-embedding-3-large", 
            "dimensions": 3072,
            "cost_per_1M": "$0.13",
            "performance": "Highest | ìµœê³ ",
            "recommended": "For high performance | ê³ ì„±ëŠ¥ í•„ìš”ì‹œ"
        },
        {
            "name": "text-embedding-ada-002",
            "dimensions": 1536, 
            "cost_per_1M": "$0.10",
            "performance": "Average | ë³´í†µ",
            "recommended": "Legacy model | êµ¬ ëª¨ë¸"
        }
    ]
    
    print(f"{'Model Name | ëª¨ë¸ëª…':<25} {'Dimensions | ì°¨ì›':<6} {'Cost | ë¹„ìš©':<8} {'Performance | ì„±ëŠ¥':<6} {'Recommendation | ì¶”ì²œ'}")
    print("-" * 65)
    
    for model in models:
        print(f"{model['name']:<25} {model['dimensions']:<6} {model['cost_per_1M']:<8} "
              f"{model['performance']:<6} {model['recommended']}")
    
    print("\nğŸ”— More Information | ë” ìì„¸í•œ ì •ë³´:")
    print("   - OpenAI Embeddings: https://platform.openai.com/docs/guides/embeddings")
    print("   - Pricing | ê°€ê²© ì •ë³´: https://openai.com/pricing")


if __name__ == "__main__":
    show_openai_embedding_models()
    main()