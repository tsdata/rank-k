"""
Reference-based evaluation example for ranx-k.

This example demonstrates the difference between retrieval-based and 
reference-based evaluation modes, showing how reference-based mode
provides more accurate assessment of retrieval completeness.
"""

# Simulated example showing the evaluation modes
print("ğŸš€ Reference-based Evaluation Example | ì°¸ì¡° ê¸°ë°˜ í‰ê°€ ì˜ˆì œ")
print("=" * 60)

# Example retriever and data setup would go here
# from your_module import retriever, questions, reference_contexts

# Simulated evaluation results for demonstration
print("\nğŸ“Š Retrieval-based Evaluation (Original) | ê²€ìƒ‰ ê¸°ë°˜ í‰ê°€ (ê¸°ì¡´):")
print("   - Evaluates only retrieved documents | ê²€ìƒ‰ëœ ë¬¸ì„œë§Œ í‰ê°€")
print("   - May show perfect scores even when missing references | ì°¸ì¡° ë¬¸ì„œë¥¼ ë†“ì³ë„ ì™„ë²½í•œ ì ìˆ˜ ê°€ëŠ¥")
print("\n   Example results | ì˜ˆì‹œ ê²°ê³¼:")
print("   hit_rate@5: 1.000")
print("   ndcg@5: 1.000")
print("   map@5: 1.000")
print("   mrr: 1.000")

print("\nğŸ“Š Reference-based Evaluation (New) | ì°¸ì¡° ê¸°ë°˜ í‰ê°€ (ì‹ ê·œ):")
print("   - Evaluates against all reference documents | ëª¨ë“  ì°¸ì¡° ë¬¸ì„œ ëŒ€ìƒ í‰ê°€")
print("   - Accurately reflects retrieval completeness | ê²€ìƒ‰ ì™„ì „ì„±ì„ ì •í™•íˆ ë°˜ì˜")
print("\n   Example results | ì˜ˆì‹œ ê²°ê³¼:")
print("   hit_rate@5: 0.900")
print("   ndcg@5: 0.876")
print("   map@5: 0.833")
print("   mrr: 0.900")
print("   recall@5: 0.833")

print("\nğŸ’¡ Key Differences | ì£¼ìš” ì°¨ì´ì :")
print("1. Recall calculation | ì¬í˜„ìœ¨ ê³„ì‚°:")
print("   - Retrieval-based | ê²€ìƒ‰ ê¸°ë°˜: recall = 1.0 (all retrieved docs are relevant)")
print("   - Reference-based | ì°¸ì¡° ê¸°ë°˜: recall = 0.833 (5/6 reference docs found)")

print("\n2. Use cases | ì‚¬ìš© ì‚¬ë¡€:")
print("   - Retrieval-based | ê²€ìƒ‰ ê¸°ë°˜: Quick quality check of retrieved results")
print("   - Reference-based | ì°¸ì¡° ê¸°ë°˜: Comprehensive evaluation for production")

# Code example
print("\nğŸ“ Code Example | ì½”ë“œ ì˜ˆì œ:")
print("""
from ranx_k.evaluation import evaluate_with_ranx_similarity

# Reference-based evaluation (recommended)
results = evaluate_with_ranx_similarity(
    retriever=your_retriever,
    questions=questions,
    reference_contexts=reference_contexts,
    k=5,
    method='kiwi_rouge',
    similarity_threshold=0.5,
    evaluation_mode='reference_based',  # NEW: Proper recall calculation
    use_graded_relevance=False         # NEW: Optional graded relevance
)

print(f"Recall@5: {results['recall@5']:.3f}")  # Now available!
""")

print("\nâœ… Recommendation | ê¶Œì¥ì‚¬í•­:")
print("   Use 'reference_based' mode for accurate evaluation!")
print("   ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´ 'reference_based' ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”!")