#!/usr/bin/env python3
"""
ranx-k ì»¤ìŠ¤í…€ ê²€ìƒ‰ê¸° êµ¬í˜„ ì˜ˆì œ

ì´ ì˜ˆì œëŠ” ranx-kì™€ í˜¸í™˜ë˜ëŠ” ì»¤ìŠ¤í…€ ê²€ìƒ‰ê¸°ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""

import numpy as np
from typing import List, Any
from ranx_k.tokenizers import KiwiTokenizer
from ranx_k.evaluation import simple_kiwi_rouge_evaluation

class Document:
    """ë¬¸ì„œ í´ë˜ìŠ¤"""
    def __init__(self, content: str, metadata: dict = None):
        self.page_content = content
        self.metadata = metadata or {}

class SimpleVectorRetriever:
    """ê°„ë‹¨í•œ ë²¡í„° ê¸°ë°˜ ê²€ìƒ‰ê¸°"""
    
    def __init__(self, documents: List[str]):
        self.documents = [Document(doc) for doc in documents]
        self.tokenizer = KiwiTokenizer(method='morphs', use_stopwords=True)
        self.vocab = self._build_vocabulary()
        self.doc_vectors = self._vectorize_documents()
    
    def _build_vocabulary(self):
        """ì „ì²´ ë¬¸ì„œì—ì„œ ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•"""
        vocab = set()
        for doc in self.documents:
            tokens = self.tokenizer.tokenize(doc.page_content)
            vocab.update(tokens)
        return {word: i for i, word in enumerate(sorted(vocab))}
    
    def _vectorize_documents(self):
        """ë¬¸ì„œë“¤ì„ ë²¡í„°ë¡œ ë³€í™˜"""
        vectors = []
        for doc in self.documents:
            vector = self._text_to_vector(doc.page_content)
            vectors.append(vector)
        return np.array(vectors)
    
    def _text_to_vector(self, text: str):
        """í…ìŠ¤íŠ¸ë¥¼ TF ë²¡í„°ë¡œ ë³€í™˜"""
        tokens = self.tokenizer.tokenize(text)
        vector = np.zeros(len(self.vocab))
        
        for token in tokens:
            if token in self.vocab:
                vector[self.vocab[token]] += 1
        
        # L2 ì •ê·œí™”
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
        
        return vector
    
    def invoke(self, query: str, top_k: int = 10) -> List[Document]:
        """ì¿¼ë¦¬ì— ëŒ€í•´ ê´€ë ¨ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰"""
        query_vector = self._text_to_vector(query)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = np.dot(self.doc_vectors, query_vector)
        
        # ìƒìœ„ kê°œ ë¬¸ì„œ ì„ íƒ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        return [self.documents[i] for i in top_indices if similarities[i] > 0]

class KeywordRetriever:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ê¸°"""
    
    def __init__(self, documents: List[str]):
        self.documents = [Document(doc) for doc in documents]
        self.tokenizer = KiwiTokenizer(method='morphs', use_stopwords=True)
    
    def invoke(self, query: str, top_k: int = 10) -> List[Document]:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜ ê²€ìƒ‰"""
        query_tokens = set(self.tokenizer.tokenize(query))
        
        scored_docs = []
        for doc in self.documents:
            doc_tokens = set(self.tokenizer.tokenize(doc.page_content))
            
            # êµì§‘í•© ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
            overlap = len(query_tokens & doc_tokens)
            if overlap > 0:
                # Jaccard ìœ ì‚¬ë„
                union = len(query_tokens | doc_tokens)
                score = overlap / union
                scored_docs.append((score, doc))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        
        return [doc for score, doc in scored_docs[:top_k]]

class HybridRetriever:
    """ë²¡í„° + í‚¤ì›Œë“œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°"""
    
    def __init__(self, documents: List[str], vector_weight: float = 0.7):
        self.vector_retriever = SimpleVectorRetriever(documents)
        self.keyword_retriever = KeywordRetriever(documents)
        self.vector_weight = vector_weight
        self.keyword_weight = 1.0 - vector_weight
    
    def invoke(self, query: str, top_k: int = 10) -> List[Document]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰"""
        # ê° ê²€ìƒ‰ê¸°ì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        vector_results = self.vector_retriever.invoke(query, top_k * 2)
        keyword_results = self.keyword_retriever.invoke(query, top_k * 2)
        
        # ë¬¸ì„œë³„ ì ìˆ˜ ì§‘ê³„
        doc_scores = {}
        
        # ë²¡í„° ê²€ìƒ‰ ì ìˆ˜
        for i, doc in enumerate(vector_results):
            score = (len(vector_results) - i) / len(vector_results)  # ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜
            doc_scores[doc.page_content] = doc_scores.get(doc.page_content, 0) + \
                                         score * self.vector_weight
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ì ìˆ˜
        for i, doc in enumerate(keyword_results):
            score = (len(keyword_results) - i) / len(keyword_results)
            doc_scores[doc.page_content] = doc_scores.get(doc.page_content, 0) + \
                                         score * self.keyword_weight
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        
        # Document ê°ì²´ ë°˜í™˜
        result_docs = []
        doc_map = {doc.page_content: doc for doc in 
                  self.vector_retriever.documents}
        
        for content, score in sorted_docs[:top_k]:
            if content in doc_map:
                result_docs.append(doc_map[content])
        
        return result_docs

def main():
    print("ğŸ” ranx-k ì»¤ìŠ¤í…€ ê²€ìƒ‰ê¸° ì˜ˆì œ")
    print("=" * 50)
    
    # ìƒ˜í”Œ ë¬¸ì„œ ì»¬ë ‰ì…˜
    documents = [
        "ìì—°ì–´ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ë¡œ ì •ë³´ ê²€ìƒ‰ê³¼ í…ìŠ¤íŠ¸ ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤.",
        "í•œêµ­ì–´ í† í°í™”ëŠ” êµì°©ì–´ì  íŠ¹ì„± ë•Œë¬¸ì— í˜•íƒœì†Œ ë¶„ì„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
        "ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì€ ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ë¡œ í‰ê°€ë©ë‹ˆë‹¤.",
        "KiwiëŠ” í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì— íŠ¹í™”ëœ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.",
        "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ì—ëŠ” ëŒ€ëŸ‰ì˜ ë¼ë²¨ë§ëœ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
        "ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "ê²€ìƒ‰ ì—”ì§„ ìµœì í™”ëŠ” ì›¹ì‚¬ì´íŠ¸ ê°€ì‹œì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.",
        "í…ìŠ¤íŠ¸ ë§ˆì´ë‹ì€ ë¹„êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ì—ì„œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.",
        "ì¶”ì²œ ì‹œìŠ¤í…œì€ ì‚¬ìš©ì ì„ í˜¸ë„ë¥¼ í•™ìŠµí•˜ì—¬ ê°œì¸í™”ëœ ì½˜í…ì¸ ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
        "ì»´í“¨í„° ë¹„ì „ ê¸°ìˆ ì€ ì´ë¯¸ì§€ì™€ ì˜ìƒì„ ë¶„ì„í•˜ê³  ì´í•´í•©ë‹ˆë‹¤.",
        "ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì—ì„œ ì¤‘ìš”í•œ ë‹¨ê³„ì…ë‹ˆë‹¤."
    ]
    
    print(f"ğŸ“š ë¬¸ì„œ ì»¬ë ‰ì…˜: {len(documents)}ê°œ ë¬¸ì„œ")
    
    # ë‹¤ì–‘í•œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
    vector_retriever = SimpleVectorRetriever(documents)
    keyword_retriever = KeywordRetriever(documents)
    hybrid_retriever = HybridRetriever(documents, vector_weight=0.6)
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "ìì—°ì–´ì²˜ë¦¬ ê¸°ìˆ ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "RAG ì‹œìŠ¤í…œì˜ íŠ¹ì§•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "í•œêµ­ì–´ í† í°í™” ë°©ë²•ì€?",
        "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ê³¼ì •ì€?"
    ]
    
    print(f"ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {len(test_queries)}ê°œ")
    
    # 1. ê° ê²€ìƒ‰ê¸°ë³„ ê²°ê³¼ ë¹„êµ
    print("\n1ï¸âƒ£ ê²€ìƒ‰ê¸°ë³„ ê²°ê³¼ ë¹„êµ")
    print("-" * 50)
    
    for i, query in enumerate(test_queries[:2]):  # ì²˜ìŒ 2ê°œë§Œ
        print(f"\nì¿¼ë¦¬ {i+1}: {query}")
        print("=" * 40)
        
        # ë²¡í„° ê²€ìƒ‰
        vector_results = vector_retriever.invoke(query, 3)
        print(f"\nğŸ”¢ ë²¡í„° ê²€ìƒ‰ ({len(vector_results)}ê°œ):")
        for j, doc in enumerate(vector_results, 1):
            print(f"  {j}. {doc.page_content[:50]}...")
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰
        keyword_results = keyword_retriever.invoke(query, 3)
        print(f"\nğŸ”¤ í‚¤ì›Œë“œ ê²€ìƒ‰ ({len(keyword_results)}ê°œ):")
        for j, doc in enumerate(keyword_results, 1):
            print(f"  {j}. {doc.page_content[:50]}...")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        hybrid_results = hybrid_retriever.invoke(query, 3)
        print(f"\nğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ({len(hybrid_results)}ê°œ):")
        for j, doc in enumerate(hybrid_results, 1):
            print(f"  {j}. {doc.page_content[:50]}...")
    
    # 2. ì„±ëŠ¥ í‰ê°€
    print("\n2ï¸âƒ£ ê²€ìƒ‰ê¸° ì„±ëŠ¥ í‰ê°€")
    print("-" * 30)
    
    # í‰ê°€ ë°ì´í„° ì¤€ë¹„
    eval_questions = [
        "ìì—°ì–´ì²˜ë¦¬ ê¸°ìˆ ì´ë€?",
        "RAG ì‹œìŠ¤í…œ ì„¤ëª…",
        "í•œêµ­ì–´ í† í°í™” íŠ¹ì§•",
        "ì •ë³´ ê²€ìƒ‰ í‰ê°€ ë°©ë²•"
    ]
    
    eval_references = [
        ["ìì—°ì–´ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì…ë‹ˆë‹¤."],
        ["RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ë¡œ ì •ë³´ ê²€ìƒ‰ê³¼ í…ìŠ¤íŠ¸ ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤."],
        ["í•œêµ­ì–´ í† í°í™”ëŠ” êµì°©ì–´ì  íŠ¹ì„± ë•Œë¬¸ì— í˜•íƒœì†Œ ë¶„ì„ì´ ì¤‘ìš”í•©ë‹ˆë‹¤."],
        ["ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì€ ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ë¡œ í‰ê°€ë©ë‹ˆë‹¤."]
    ]
    
    retrievers = [
        ("ë²¡í„° ê²€ìƒ‰ê¸°", vector_retriever),
        ("í‚¤ì›Œë“œ ê²€ìƒ‰ê¸°", keyword_retriever),  
        ("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°", hybrid_retriever)
    ]
    
    print(f"{'ê²€ìƒ‰ê¸°':<15} {'ROUGE-1':<10} {'ROUGE-2':<10} {'ROUGE-L':<10}")
    print("-" * 50)
    
    for name, retriever in retrievers:
        try:
            results = simple_kiwi_rouge_evaluation(
                retriever=retriever,
                questions=eval_questions,
                reference_contexts=eval_references,
                k=3
            )
            
            rouge1 = results.get('kiwi_rouge1@3', 0.0)
            rouge2 = results.get('kiwi_rouge2@3', 0.0)
            rougeL = results.get('kiwi_rougeL@3', 0.0)
            
            print(f"{name:<15} {rouge1:<10.3f} {rouge2:<10.3f} {rougeL:<10.3f}")
            
        except Exception as e:
            print(f"{name:<15} ì˜¤ë¥˜: {str(e)}")
    
    # 3. ì„±ëŠ¥ ë¶„ì„
    print("\n3ï¸âƒ£ ì„±ëŠ¥ íŠ¹ì„± ë¶„ì„")
    print("-" * 25)
    
    analysis_query = "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€?"
    
    print(f"ë¶„ì„ ì¿¼ë¦¬: {analysis_query}")
    print("\nê° ê²€ìƒ‰ê¸°ì˜ íŠ¹ì„±:")
    
    # ë²¡í„° ê²€ìƒ‰ê¸° ë¶„ì„
    vector_results = vector_retriever.invoke(analysis_query, 5)
    print(f"\nğŸ”¢ ë²¡í„° ê²€ìƒ‰ê¸°:")
    print(f"  - ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜")
    print(f"  - ê²€ìƒ‰ ê²°ê³¼: {len(vector_results)}ê°œ")
    print(f"  - ì¥ì : ë™ì˜ì–´, ìœ ì‚¬ ê°œë… ê²€ìƒ‰ ê°€ëŠ¥")
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰ê¸° ë¶„ì„
    keyword_results = keyword_retriever.invoke(analysis_query, 5)
    print(f"\nğŸ”¤ í‚¤ì›Œë“œ ê²€ìƒ‰ê¸°:")
    print(f"  - ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­")
    print(f"  - ê²€ìƒ‰ ê²°ê³¼: {len(keyword_results)}ê°œ")
    print(f"  - ì¥ì : ë¹ ë¥¸ ì†ë„, ì •í™•í•œ ë§¤ì¹­")
    
    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ë¶„ì„
    hybrid_results = hybrid_retriever.invoke(analysis_query, 5)
    print(f"\nğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°:")
    print(f"  - ë²¡í„° + í‚¤ì›Œë“œ ê²°í•©")
    print(f"  - ê²€ìƒ‰ ê²°ê³¼: {len(hybrid_results)}ê°œ")
    print(f"  - ì¥ì : ë‘ ë°©ì‹ì˜ ì¥ì  ê²°í•©")
    
    print("\nâœ… ì»¤ìŠ¤í…€ ê²€ìƒ‰ê¸° ì˜ˆì œ ì™„ë£Œ!")
    print("\nğŸ’¡ êµ¬í˜„ ê°€ì´ë“œ:")
    print("1. invoke(query) ë©”ì„œë“œë§Œ êµ¬í˜„í•˜ë©´ ranx-kì™€ í˜¸í™˜")
    print("2. Document ê°ì²´ì˜ page_content ì†ì„± í•„ìš”")
    print("3. ë‹¤ì–‘í•œ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì¡°í•© ê°€ëŠ¥")

if __name__ == "__main__":
    main()