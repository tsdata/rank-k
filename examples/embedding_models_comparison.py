#!/usr/bin/env python3

"""
Example: Comparing Different Embedding Models for RAG Evaluation

This example demonstrates how to use different embedding models 
for similarity-based ranx evaluation in Korean RAG systems.
"""

import warnings
warnings.filterwarnings("ignore", message=".*encoder_attention_mask.*")

from ranx_k.evaluation.similarity_ranx import evaluate_with_ranx_similarity

def compare_embedding_models(retriever, questions, reference_contexts, k=5):
    """
    Compare different embedding models for RAG evaluation.
    
    Args:
        retriever: RAG retriever object
        questions: List of evaluation questions
        reference_contexts: List of reference document lists
        k: Number of top documents to evaluate
    
    Returns:
        dict: Comparison results for different models
    """
    
    print("ğŸ” Embedding Models Comparison for RAG Evaluation | ì„ë² ë”© ëª¨ë¸ ë¹„êµ í‰ê°€")
    print("="*80)
    
    # Define embedding models to compare
    embedding_models = {
        "MiniLM-L12-v2": {
            "model": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            "threshold": 0.6,
            "description": "Fast and lightweight multilingual model | ë¹ ë¥´ê³  ê°€ë²¼ìš´ ë‹¤êµ­ì–´ ëª¨ë¸"
        },
        "OpenAI-3-Small": {
            "model": "text-embedding-3-small",
            "method": "openai",
            "threshold": 0.7,
            "description": "OpenAI embedding model (requires API key) | OpenAI ì„ë² ë”© ëª¨ë¸ (API í‚¤ í•„ìš”)"
        },
        "BGE-M3": {
            "model": "BAAI/bge-m3",
            "threshold": 0.6,
            "description": "Latest multilingual embedding model | ìµœì‹  ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸"
        },
        "All-MPNet-Base-v2": {
            "model": "sentence-transformers/all-mpnet-base-v2",
            "threshold": 0.7,
            "description": "High-quality English model | ê³ í’ˆì§ˆ ì˜ì–´ ëª¨ë¸"
        }
    }
    
    all_results = {}
    
    for model_name, config in embedding_models.items():
        print(f"\nğŸš€ Testing {model_name} | {model_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
        print(f"ğŸ“ {config['description']}")
        print(f"ğŸ¯ Model: {config['model']}")
        print(f"ğŸ¯ Threshold: {config['threshold']}")
        print("-" * 60)
        
        try:
            method = config.get('method', 'embedding')
            results = evaluate_with_ranx_similarity(
                retriever=retriever,
                questions=questions,
                reference_contexts=reference_contexts,
                k=k,
                method=method,
                similarity_threshold=config['threshold'],
                embedding_model=config['model']
            )
            
            if results:
                all_results[model_name] = results
                print(f"âœ… {model_name} evaluation completed | {model_name} í‰ê°€ ì™„ë£Œ")
            else:
                print(f"âŒ {model_name} evaluation failed | {model_name} í‰ê°€ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ Error with {model_name}: {e}")
            print(f"ğŸ’¡ Try installing model: pip install sentence-transformers")
    
    # Results comparison
    if all_results:
        print("\nğŸ† Embedding Models Performance Comparison | ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        print("="*80)
        
        # Create comparison table
        metrics = ['hit_rate@5', 'ndcg@5', 'map@5', 'mrr']
        
        # Header
        print(f"{'Model':<20} {'Hit@5':<10} {'NDCG@5':<10} {'MAP@5':<10} {'MRR':<10}")
        print("-" * 70)
        
        # Results for each model
        for model_name, results in all_results.items():
            row = f"{model_name:<20}"
            for metric in metrics:
                if metric in results:
                    row += f"{results[metric]:<10.3f}"
                else:
                    row += f"{'N/A':<10}"
            print(row)
        
        # Find best model
        best_model = None
        best_score = 0
        
        for model_name, results in all_results.items():
            if 'hit_rate@5' in results and results['hit_rate@5'] > best_score:
                best_score = results['hit_rate@5']
                best_model = model_name
        
        if best_model:
            print(f"\nğŸ¥‡ Best Model | ìµœê³  ëª¨ë¸: {best_model} (Hit@5: {best_score:.3f})")
    
    return all_results

def example_with_korean_text():
    """
    Example with Korean text data for demonstration.
    """
    print("\nğŸ” Korean Text Example | í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì˜ˆì œ")
    print("="*50)
    
    # Sample Korean RAG data
    questions = [
        "TeslaëŠ” ì–´ë–¤ íšŒì‚¬ì¸ê°€ìš”?",
        "ì „ê¸°ì°¨ì˜ ì£¼ìš” ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ììœ¨ì£¼í–‰ ê¸°ìˆ ì˜ í˜„ì¬ ìˆ˜ì¤€ì€?",
    ]
    
    # Sample reference contexts (what should be retrieved)
    reference_contexts = [
        ["TeslaëŠ” ë¯¸êµ­ì˜ ì „ê¸°ìë™ì°¨ ë° ì²­ì •ì—ë„ˆì§€ íšŒì‚¬ì…ë‹ˆë‹¤."],
        ["ì „ê¸°ì°¨ëŠ” í™˜ê²½ì¹œí™”ì ì´ê³  ì—°ë£Œë¹„ê°€ ì ˆì•½ë˜ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤."],
        ["ììœ¨ì£¼í–‰ ê¸°ìˆ ì€ ë ˆë²¨ 2-3 ìˆ˜ì¤€ì—ì„œ ìƒìš©í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤."],
    ]
    
    # Mock retriever for demonstration (replace with your actual retriever)
    class MockRetriever:
        def invoke(self, question):
            # Mock retrieved documents
            from types import SimpleNamespace
            docs = [
                SimpleNamespace(page_content="TeslaëŠ” ì „ê¸°ìë™ì°¨ë¥¼ ë§Œë“œëŠ” ë¯¸êµ­ íšŒì‚¬ì…ë‹ˆë‹¤."),
                SimpleNamespace(page_content="ì „ê¸°ì°¨ëŠ” ë°°í„°ë¦¬ë¡œ êµ¬ë™ë˜ëŠ” ì¹œí™˜ê²½ ì°¨ëŸ‰ì…ë‹ˆë‹¤."),
                SimpleNamespace(page_content="ììœ¨ì£¼í–‰ì€ AI ê¸°ìˆ ì„ í™œìš©í•œ ìš´ì „ ë³´ì¡° ì‹œìŠ¤í…œì…ë‹ˆë‹¤."),
                SimpleNamespace(page_content="ì „í˜€ ê´€ë ¨ì—†ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤."),
                SimpleNamespace(page_content="ë‹¤ë¥¸ ì£¼ì œì— ëŒ€í•œ ë¬¸ì„œì…ë‹ˆë‹¤."),
            ]
            return docs
    
    mock_retriever = MockRetriever()
    
    # Convert reference contexts to Document objects
    from types import SimpleNamespace
    formatted_references = []
    for ref_list in reference_contexts:
        docs = [SimpleNamespace(page_content=ref) for ref in ref_list]
        formatted_references.append(docs)
    
    # Test with different embedding models
    print("Testing different embedding models with Korean text...")
    
    results = evaluate_with_ranx_similarity(
        retriever=mock_retriever,
        questions=questions,
        reference_contexts=formatted_references,
        k=5,
        method='embedding',
        similarity_threshold=0.5,
        embedding_model="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )
    
    print(f"ğŸ“Š Results | ê²°ê³¼:")
    for metric, score in results.items():
        print(f"  {metric}: {score:.3f}")

if __name__ == "__main__":
    print("ğŸš€ Embedding Models Comparison Example | ì„ë² ë”© ëª¨ë¸ ë¹„êµ ì˜ˆì œ")
    print("="*80)
    
    # Run Korean text example
    example_with_korean_text()
    
    print(f"\nğŸ’¡ Usage Tips | ì‚¬ìš© íŒ:")
    print(f"  - Use higher thresholds (0.6-0.8) for more accurate models")
    print(f"  - BGE-M3 works well for Korean and multilingual content")
    print(f"  - MPNet models provide good balance of accuracy and speed")
    print(f"  - ë” ì •í™•í•œ ëª¨ë¸ì—ëŠ” ë†’ì€ ì„ê³„ê°’(0.6-0.8)ì„ ì‚¬ìš©í•˜ì„¸ìš”")
    print(f"  - BGE-M3ëŠ” í•œêµ­ì–´ì™€ ë‹¤êµ­ì–´ ì½˜í…ì¸ ì— ì í•©í•©ë‹ˆë‹¤")
    print(f"  - MPNet ëª¨ë¸ì€ ì •í™•ë„ì™€ ì†ë„ì˜ ì¢‹ì€ ê· í˜•ì„ ì œê³µí•©ë‹ˆë‹¤")