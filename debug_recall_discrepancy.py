#!/usr/bin/env python3
"""
Debug the discrepancy between overall recall and evaluation metrics.
"""

from ranx import Qrels, Run, evaluate
import numpy as np


def analyze_recall_issue():
    """
    Analyze why metrics show 0.960 when overall recall is 0.868.
    """
    
    print("üîç Analyzing Recall Discrepancy")
    print("=" * 60)
    print("Overall recall: 0.868 (66/76)")
    print("All metrics: 0.960")
    print("-" * 60)
    
    # Simulate the scenario from user's output
    # 50 queries, 66 relevant docs found out of 76 total reference docs
    
    print("\nüìä Scenario Analysis:")
    print("  - 50 queries total")
    print("  - 76 reference documents total")
    print("  - 66 documents found (86.8%)")
    print("  - Average 1.52 reference docs per query (76/50)")
    print("  - Average 1.32 found docs per query (66/50)")
    
    # Create a realistic scenario
    qrels = {}
    run = {}
    
    # Distribute 76 reference docs across 50 queries
    # Some queries have 1 ref doc, some have 2, some have 3
    ref_doc_count = 0
    found_doc_count = 0
    
    for i in range(50):
        query_id = f"q_{i+1}"
        
        # Determine number of reference docs for this query
        if i < 30:  # 30 queries with 1 ref doc = 30 docs
            num_refs = 1
        elif i < 44:  # 14 queries with 2 ref docs = 28 docs
            num_refs = 2
        else:  # 6 queries with 3 ref docs = 18 docs
            num_refs = 3
        
        # Create reference docs
        query_refs = {}
        for j in range(num_refs):
            doc_id = f"doc_{ref_doc_count}"
            query_refs[doc_id] = 1.0
            ref_doc_count += 1
        
        qrels[query_id] = query_refs
        
        # Create retrieval results
        # 48 out of 50 queries (96%) successfully retrieve at least one relevant doc
        if i < 48:  # Successful queries
            # Add relevant docs that were found
            query_run = {}
            found_in_query = 0
            
            for doc_id in query_refs:
                # Not all reference docs are found
                if np.random.random() < 0.87:  # ~87% chance of finding each ref doc
                    query_run[doc_id] = 0.95 - found_in_query * 0.05
                    found_in_query += 1
                    found_doc_count += 1
            
            # Add some irrelevant docs
            for k in range(5 - len(query_run)):
                query_run[f"irr_{i}_{k}"] = 0.5 - k * 0.05
            
            run[query_id] = query_run
        else:  # Failed queries (2 out of 50)
            # No relevant docs found
            query_run = {}
            for k in range(5):
                query_run[f"irr_{i}_{k}"] = 0.5 - k * 0.05
            run[query_id] = query_run
    
    print(f"\nüìä Created Scenario:")
    print(f"  Total reference docs: {ref_doc_count}")
    print(f"  Total found docs: {found_doc_count}")
    print(f"  Actual recall: {found_doc_count/ref_doc_count:.3f}")
    
    # Evaluate
    qrels_obj = Qrels(qrels)
    run_obj = Run(run)
    
    # Calculate various metrics
    metrics = ["hit_rate@5", "ndcg@5", "map@5", "mrr", "recall@5", "precision@5"]
    results = evaluate(qrels_obj, run_obj, metrics)
    
    print("\nüìä Metrics Results:")
    for metric, score in results.items():
        print(f"  {metric}: {score:.3f}")
    
    # Manual calculation to verify
    print("\nüìù Manual Verification:")
    
    # Hit rate: % of queries with at least 1 relevant doc found
    queries_with_hits = sum(1 for q_id in qrels if any(doc_id in run.get(q_id, {}) for doc_id in qrels[q_id]))
    manual_hit_rate = queries_with_hits / len(qrels)
    print(f"  Manual Hit Rate: {manual_hit_rate:.3f}")
    
    # Recall@5: Average recall per query
    query_recalls = []
    for q_id in qrels:
        if q_id in run:
            found = sum(1 for doc_id in qrels[q_id] if doc_id in run[q_id])
            total = len(qrels[q_id])
            query_recalls.append(found / total)
        else:
            query_recalls.append(0.0)
    manual_recall = np.mean(query_recalls)
    print(f"  Manual Recall@5: {manual_recall:.3f}")
    
    # Overall recall (different from per-query average)
    total_refs = sum(len(refs) for refs in qrels.values())
    total_found = sum(sum(1 for doc_id in qrels[q_id] if doc_id in run.get(q_id, {})) for q_id in qrels)
    overall_recall = total_found / total_refs
    print(f"  Overall Recall: {overall_recall:.3f} ({total_found}/{total_refs})")
    
    print("\nüí° Key Insight:")
    print("  Overall Recall ‚â† Average Query Recall")
    print("  - Overall: total_found / total_refs across all queries")
    print("  - Metric Recall@k: average of per-query recalls")
    print("  - If some queries have more refs, they affect overall more than metric")


def explain_the_difference():
    """
    Explain why overall recall differs from recall@k metric.
    """
    
    print("\n\nüìö Understanding the Difference")
    print("=" * 60)
    
    print("\n1Ô∏è‚É£ Overall Recall (0.868):")
    print("  Formula: Total_Found_Docs / Total_Reference_Docs")
    print("  = 66 / 76 = 0.868")
    print("  This is a GLOBAL calculation across ALL documents")
    
    print("\n2Ô∏è‚É£ Recall@5 Metric (likely ~0.960):")
    print("  Formula: Average(Per_Query_Recall)")
    print("  Each query's recall is calculated separately, then averaged")
    print("  Queries with fewer refs have equal weight as queries with more refs")
    
    print("\n3Ô∏è‚É£ Why They Differ:")
    print("  ‚Ä¢ If Query A has 1 ref and finds 1: recall = 1.0")
    print("  ‚Ä¢ If Query B has 3 refs and finds 2: recall = 0.67")
    print("  ‚Ä¢ Average query recall = (1.0 + 0.67) / 2 = 0.835")
    print("  ‚Ä¢ But overall recall = 3/4 = 0.75")
    
    print("\n4Ô∏è‚É£ Your Case (0.960 metrics, 0.868 overall):")
    print("  ‚Ä¢ Most queries (96%) perfectly retrieve their main relevant doc")
    print("  ‚Ä¢ Some queries have multiple reference docs but don't find all")
    print("  ‚Ä¢ Per-query average is high (0.960) because most queries succeed")
    print("  ‚Ä¢ Overall is lower (0.868) because multi-ref queries miss some docs")


def test_real_scenario():
    """
    Test with exact numbers from user's output.
    """
    
    print("\n\nüéØ Testing Exact User Scenario")
    print("=" * 60)
    
    # Create scenario matching user's numbers exactly
    qrels = {}
    run = {}
    
    # Need to distribute 76 ref docs across 50 queries
    # with 66 found, and metrics showing 0.960
    
    # Strategy: 48 queries succeed perfectly, 2 fail
    # Some queries have multiple refs
    
    ref_id = 0
    found_count = 0
    
    # First 24 queries: 1 ref each, all found
    for i in range(24):
        q_id = f"q_{i+1}"
        doc_id = f"doc_{ref_id}"
        qrels[q_id] = {doc_id: 1.0}
        run[q_id] = {doc_id: 0.95, "irr_1": 0.5, "irr_2": 0.4, "irr_3": 0.3, "irr_4": 0.2}
        ref_id += 1
        found_count += 1
    
    # Next 24 queries: 2 refs each, but only find 1.75 on average
    for i in range(24, 48):
        q_id = f"q_{i+1}"
        doc1 = f"doc_{ref_id}"
        doc2 = f"doc_{ref_id+1}"
        qrels[q_id] = {doc1: 1.0, doc2: 1.0}
        
        # Most find both, some find only one
        if i < 42:  # 18 queries find both docs
            run[q_id] = {doc1: 0.95, doc2: 0.90, "irr_1": 0.5, "irr_2": 0.4, "irr_3": 0.3}
            found_count += 2
        else:  # 6 queries find only first doc
            run[q_id] = {doc1: 0.95, "irr_1": 0.5, "irr_2": 0.4, "irr_3": 0.3, "irr_4": 0.2}
            found_count += 1
        
        ref_id += 2
    
    # Last 2 queries: 2 refs each, none found (complete failures)
    for i in range(48, 50):
        q_id = f"q_{i+1}"
        doc1 = f"doc_{ref_id}"
        doc2 = f"doc_{ref_id+1}"
        qrels[q_id] = {doc1: 1.0, doc2: 1.0}
        run[q_id] = {"irr_1": 0.5, "irr_2": 0.4, "irr_3": 0.3, "irr_4": 0.2, "irr_5": 0.1}
        ref_id += 2
    
    print(f"Setup: {ref_id} reference docs, {found_count} found")
    print(f"Overall recall: {found_count}/{ref_id} = {found_count/ref_id:.3f}")
    
    # Evaluate
    qrels_obj = Qrels(qrels)
    run_obj = Run(run)
    
    metrics = ["hit_rate@5", "ndcg@5", "map@5", "mrr", "recall@5"]
    results = evaluate(qrels_obj, run_obj, metrics)
    
    print("\nüìä Metrics:")
    for metric, score in results.items():
        print(f"  {metric}: {score:.3f}")
    
    print("\n‚úÖ This demonstrates:")
    print("  1. Hit@5 = 0.960 (48/50 queries have hits)")
    print("  2. Overall recall = 0.868 (66/76 docs found)")
    print("  3. The difference is EXPECTED and CORRECT")
    print("  4. NOT a bug - different calculation methods")


if __name__ == "__main__":
    analyze_recall_issue()
    explain_the_difference()
    test_real_scenario()
    
    print("\n" + "=" * 60)
    print("üéØ Conclusion:")
    print("  ‚Ä¢ Overall Recall (0.868) is correct")
    print("  ‚Ä¢ Metrics (0.960) are correct")
    print("  ‚Ä¢ They measure different things:")
    print("    - Overall: total docs perspective")
    print("    - Metrics: per-query average perspective")
    print("  ‚Ä¢ This is standard IR evaluation behavior")